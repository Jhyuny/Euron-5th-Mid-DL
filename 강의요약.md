# ê°•ì˜ ìš”ì•½

## ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•

- ë²¡í„°í™” : mê°œì˜ ìƒ˜í”Œì— ëŒ€í•œ ê³„ì‚° (ë°˜ë³µë¬¸ ì—†ì´)
    
    â†’ í›ˆë ¨ ìƒ˜í”Œì— ëŒ€í•´ ë²¡í„°í™”í•œ í›„ì— ë²¡í„° ê°„ì˜ ê³„ì‚°ì„ ì§„í–‰í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    
    ì´ ë•Œ mì´ ë§¤ìš° í¬ë‹¤ë©´ ê·¸ ì†ë„ëŠ” ì—¬ì „íˆ ëŠë¦¬ë‹¤. ë‹¨ê³„ì˜ ë°˜ë³µì„ ìœ„í•´ ë” ì‘ì€ í›ˆë ¨ ì„¸íŠ¸ë¡œ ë‚˜ëˆˆë‹¤ â†’ mini-batch gradient descent
    
    ```python
    # Mini-batch gradient descent
    for i in range(1,5001):
    	forward prop on X^{t}
    		Z[i] = W[i]X[t]+b[i]
    			A[1]=g[1](Z[1]) ... # Lë²ˆì§¸ ê¹Œì§€
    	compute cost J = 1/1000 sum(L(y,y')) + (ì •ê·œí™”í•­)
    	///
    	backprop to compute gradients 
    # ì´ 5000ë²ˆì˜ epochì„ í•™ìŠµ
    ```
    

## ****ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²• ì´í•´í•˜ê¸°****

- batch gradient descent : ëª¨ë“  ë°˜ë³µì—ì„œ ëª¨ë“  í›ˆë ¨ ì„¸íŠ¸ë¥¼ í•™ìŠµ
mini-batch gradient descent : ì¼ë¶€ ì„¸íŠ¸ë§Œ í•™ìŠµí•˜ë©° ë°˜ë³µ â†’ í•­ìƒ í•˜í–¥í•˜ì§€ ì•ŠìŒ(ê²½í–¥ì„±ì„ ë°)

![Untitled](ìë£Œ/Untitled.png)

- ë¯¸ë‹ˆë°°ì¹˜ì˜ í¬ê¸°
    - ìµœëŒ€ í›ˆë ¨ì„¸íŠ¸ ìˆ˜(m)ì¸ ê²½ìš° : ë„ˆë¬´ ê¸¸ê³  ì˜¤ë˜ ê±¸ë¦¼
    - size1ê°€ 1ì¸ ê²½ìš°(SGD)  : ì‘ì€ í•™ìŠµë¥ ë¡œ ë…¸ì´ì¦ˆë¥¼ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë‚˜, í•œ ë²ˆì— í•˜ë‚˜ì˜ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ë¹„íš¨ìœ¨ì 
    
    â†’ 1ê³¼ mì‚¬ì´ì˜ ì ì ˆí•œ ê°’ì„ ì§€ì •í•˜ì—¬ ì‚¬ìš©, í•­ìƒ ìµœì†Ÿê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§„ ì•Šì§€ë§Œ ì¼ê´€ë˜ê²Œ global minimumìœ¼ë¡œ í–¥í•˜ëŠ” ê²½í–¥ì„ ë³´ì„
    
- í¬ê¸° ì„¤ì •í•˜ê¸°
    - í›ˆë ¨ ì„¸íŠ¸ê°€ ì‘ìœ¼ë©´ ê·¸ëƒ¥ GDë¥¼ ì‚¬ìš©(1 iteration = 1 epoch)
    - í›ˆë ¨ ì„¸íŠ¸ê°€ í¬ë©´ ì¼ë°˜ì ìœ¼ë¡œ 64~512ë¥¼ ìì£¼ ì‚¬ìš© ( ì»´í“¨í„° êµ¬ì¡° ìƒ 2ì˜ ì œê³±ì¸ ê°’ì´ ê°€ì¥ íš¨ìœ¨ì ) â†’ ê·¸ ì¤‘ ê°€ì¥ ë¹„ìš©í•¨ìˆ˜ê°€ ì‘ì€ ê°’ì„ ì±„íƒ

## ****ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ****

- ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê·  (Exponentially Weighted Average)
    
    ì´ì „ ê°’ê³¼ í˜„ì¬ ê°’ì˜ ë¹„ìœ¨ì„ ì§€ì •í•´ì„œ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹
    
    ìµœê·¼ì˜ ë°ì´í„°ì— ë” ë§ì€ ì˜í–¥ì„ ë°›ëŠ” ë°ì´í„°ë“¤ì˜ í‰ê·  íë¦„ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ì„ êµ¬í•œë‹¤
    
    ![Untitled](ìë£Œ/Untitled%201.png)
    
    ë² íƒ€ê°€ ì»¤ì§ˆìˆ˜ë¡ ê·¼ì‚¬ëœ ë‹¨ì¡°ë¡œìš´ ê³¡ì„ ì´ ë¨
    

## ****ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê·  ì´í•´í•˜ê¸°****

- Vnì„ ì„¸íƒ€ë¡œ í‘œí˜„í•˜ê¸° â†’ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ìˆ˜í•˜ëŠ” í•¨ìˆ˜ê¼´ì´ ë¨

![Untitled](ìë£Œ/Untitled%202.png)

## ****ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· ì˜ í¸í–¥ë³´ì •****

- í¸í–¥ ë³´ì •ì„ í†µí•´ ë” ì •í™•í•œ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
- Vtë¥¼ (1-ë² íƒ€^t)ë¡œ ë‚˜ëˆ„ì–´ì£¼ë©´ì„œ ì´ˆê¸° ê°’ì„ ì‹¤ì œê°’ê³¼ ë¹„ìŠ·í•˜ê²Œ í•˜ë‚˜ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ë¶„ëª¨ê°€ 1ì— ê°€ê¹Œì›Œì§€ë©´ì„œ ì¼ì¹˜í•˜ê²Œ ë¨. ì´ˆê¸°ê°’ì´ ì¤‘ìš”í•  ê²½ìš°ì—ë§Œ êµ¬í˜„í•œë‹¤.

## ****Momentum ìµœì í™” ì•Œê³ ë¦¬ì¦˜****

![Untitled](ìë£Œ/Untitled%203.png)

- ì§„ë™í•˜ë©´ì„œ ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¬ê³ , overshuttingì— ì˜í•œ ë°œì‚°ë„ ê³ ë ¤í•˜ì—¬ í•™ìŠµë¥ ì„ ì ì ˆíˆ ì¡°ì •í•´ì•¼í•œë‹¤.
- Momentumì„ ì´ìš©í•œ gradient descentì—ì„œëŠ” mini batchì— ëŒ€í•œ dw, dbë¥¼ ì´ìš©í•´ì„œ ì¡°ì ˆí•œë‹¤
    
    ![Untitled](ìë£Œ/Untitled%204.png)
    
    ìœ„ì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ê±°ì¹˜ë©° wë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.
    
    ì´ë¥¼ ì´ìš©í•œ ì—…ë°ì´íŠ¸ëŠ” ë§¤ ë‹¨ê³„ì˜ ê²½ì‚¬ í•˜ê°• ì •ë„ë¥¼ ì™„ë§Œí•˜ê²Œ ë§Œë“ ë‹¤.
    
- êµ¬í˜„ ë°©ë²•

![Untitled](ìë£Œ/Untitled%205.png)

## ****RMSProp(root mean squre) ìµœì í™” ì•Œê³ ë¦¬ì¦˜****

- Sdwë¥¼ ì´ìš©
    
    ![Untitled](ìë£Œ/Untitled%206.png)
    
    ë„í•¨ìˆ˜ì˜ ì œê³±ì„ ê°€ì¤‘í‰ê·  â†’ í´ìˆ˜ë¡ ì—…ë°ì´íŠ¸ ì‹œ ë” í° ê°’ìœ¼ë¡œ ë‚˜ëˆ ì£¼ê¸° ë•Œë¬¸ì— ê¸°ì¡´ í•™ìŠµë¥ ë³´ë‹¤ ë” ì‘ì€ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ëœë‹¤. ë°˜ëŒ€ë¡œ ë¯¸ë¶„ê°’ì´ ì‘ì€ ê³³ì—ì„œëŠ” ì—…ë°ì´íŠ¸ ì‹œ ì‘ì€ ê°’ìœ¼ë¡œ ë‚˜ëˆ ì£¼ê¸° ë•Œë¬¸ì— ê¸°ì¡´ í•™ìŠµë¥ ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ëœë‹¤. ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” íš¨ê³¼ë¥¼ ë³´ì´ë©° ê·¸ëƒ¥ Momentum ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ì—…ë°ì´íŠ¸ ì‹œì˜ ì§„ë™ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ëœë‹¤. 
    

## ****Adam(Adaptve moment estimation) ìµœì í™” ì•Œê³ ë¦¬ì¦˜****

- Momentumê³¼ RMSPropì„ ì„ì€ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ê°–ëŠ”ë‹¤

![Untitled](ìë£Œ/Untitled%207.png)

- momentumì— ëŒ€í•œ ë² íƒ€1, RMSì— ëŒ€í•œ ë² íƒ€2 ê°ê°ì„ ê³ ë ¤
    - ê°ê° 0.9, 0.99ë¥¼ ì¶”ì²œí•œë‹¤.

## ****í•™ìŠµë¥  ê°ì‡  (learning rate decay)****

- ì‹œê°„ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì ì  ì¤„ì´ëŠ” ê²ƒ
    - í•™ìŠµ ì´ˆê¸°ì— í›¨ì”¬ í° ìŠ¤í…ìœ¼ë¡œ ì§„í–‰í•˜ê³ , í•™ìŠµì´ ìˆ˜ë ´í• ìˆ˜ë¡ í•™ìŠµë¥ ì´ ëŠë ¤ì € ì‘ì€ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰

<aside>
ğŸ’¡ GPT

1. **Step Decay:**
    - The learning rate **is reduced by a constant factor** after a fixed number of training epochs or steps.
    - Formula: lr=initial_lrÃ—decay_factorfloor(step_sizeepoch)
        
        lr=initial_lrÃ—decay_factorfloor(epochstep_size)
        
2. **Exponential Decay:**
    - The learning rate **is reduced exponentially over time.**
    - Formula: lr=initial_lrÃ—exp(âˆ’decay_rateÃ—epoch)
        
        lr=initial_lrÃ—exp(âˆ’decay_rateÃ—epoch)
        
3. **Inverse Decay:**
    - The learning rate **is decreased proportionally to the inverse of the epoch number.**
    - epochì´ ì»¤ì§ˆ ìˆ˜ë¡ ì‘ì•„ì§ â†’ ì—…ë°ì´íŠ¸ ì¡°ê¸ˆì”© ì§„í–‰
    - Formula: lr=1+decay_rateÃ—epochinitial_lr
        
        lr=initial_lr1+decay_rateÃ—epoch
        
4. **Polynomial Decay:**
    - The learning rate **is reduced according to a polynomial function of the form** lr=initial_lrÃ—(1âˆ’max_epochsepoch)power
        
        lr=initial_lrÃ—(1âˆ’epochmax_epochs)power
        
5. ****Cosine Annealing Decay:(ê°€ì¥ ë§ì´ ì‚¬ìš©)**
    - The **learning rate follows a cosine function**, decreasing in a smooth curve.
    - Formula
    
    ![Untitled](ìë£Œ/Untitled%208.png)
    
6. **Warmup and Decay:**
    - A combination of a warm-up phase where the learning rate **starts low and gradually increases, followed by a decay phase where it decreases**.
    - This helps the model converge quickly at the beginning and then fine-tune with a lower learning rate.
7. **Adaptive Methods:**
    - Adaptive learning rate methods, such as **Adam or Adagrad**, adapt the learning rate for **each parameter individually based on their historical gradients**. While these methods do not have a fixed decay schedule, they implicitly adjust the learning rates during training.
</aside>